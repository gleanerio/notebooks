{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Concepts \n",
    "\n",
    "## Review\n",
    "\n",
    "Before we starting pulling data lets revist the publishing overview \n",
    "presented in [Publisher](../../publishing/publishing.md) section.\n",
    "\n",
    "![overview](../docs/images/example1Flow.png)\n",
    "\n",
    "Briefly, a sitemap is made available that points the resources we will \n",
    "be indexing.  This can optionally be in a robots.txt file as well.  \n",
    "This sitemap will provide a URL for each resource we will be indexing.  \n",
    "It is fine if it providers more too, those resources simply wont express \n",
    "any JSON-LD content.  You can also have multiple sitemaps we ones specificly\n",
    "focused on the resources to index.  \n",
    "\n",
    "Each URL or page represented in the index are then accessed and parsed for the \n",
    "JSON-LD content.  \n",
    "\n",
    "## Command Line Tooling\n",
    "\n",
    "We can start exploring the indexing of data on the command line.  Here we will use the curl command which should \n",
    "be installed on all Mac OS X and Linux systems and should be found on the Linux Subsystem for Windows. \n",
    "\n",
    "This will give us a low level feel for what is going on.  \n",
    "\n",
    "We will start by exploring a sitemap.  \n",
    "\n",
    "```bash\n",
    "curl -s https://samples.earth/sitemap0.xml\n",
    "```\n",
    "\n",
    "We can parse out the URLs from the sitemap with the use of the UNIX grep command\n",
    "\n",
    "```bash\n",
    "curl -s https://samples.earth/sitemap0.xml |   grep -oP '<loc>\\K[^<]*'\n",
    "```\n",
    "\n",
    "In do this we see that out sitemap is really just a feed of URLs.  The sitemap provides us with the ability to add \n",
    "some extra information for our URLs.  It also providers a machine readable XML format we can work with.  There are many\n",
    "libraries for working with XML and several for working with the sitemap data model in XML as well. \n",
    "\n",
    "Now lets pull down the URL resource and parse out the JSON-LD we find in the ```<script>``` tag of type application/ld+json.\n",
    "\n",
    "Note, it is possible that there are many of these tags and also that this tag might be placed in by Javascript which means \n",
    "we would not see it here.  We will talk more about this as we explore.  For now we will just look for the first one and \n",
    "a static example, which we know this to be.  \n",
    "\n",
    "```bash\n",
    "curl -s  --header \"Accept: text/html\"   https://samples.earth/id/documents/c1pnht3h2h44frv6igfg | sed -n '/<script type=\\\"application\\/ld+json\\\">/,/<\\/script>/p'\n",
    "```\n",
    "\n",
    "Let's get rid of the ```<script> ``` tags and then parse the JSON-LD. \n",
    "\n",
    "```bash\n",
    "curl -s  --header \"Accept: text/html\"   https://samples.earth/id/documents/c1pnht3h2h44frv6igfg | sed -n '/<script type=\\\"application\\/ld+json\\\">/,/<\\/script>/p' | sed 's/<\\/script>//' | sed 's/<script type=\\\"application\\/ld+json\\\">//'\n",
    "```\n",
    "\n",
    "TODO WORK ON THIS\n",
    "At this point we could copy this JOSN-LD and visit something like the JSON-LD playground.   We could also \n",
    "visit the Structure Data Linter.   We can then play a bit with the JSON-LD there.\n",
    "\n",
    "\n",
    "Now, lets pass this throgh another app.  This is the jsonld.js app.  A Javascript app and library that can be found\n",
    "on GitHub at [digitalbazaar/jsonld.js](https://github.com/digitalbazaar/jsonld.js).   There are many similar libraries,\n",
    "so you can feel free to try out others.  \n",
    "\n",
    "\n",
    "```bash\n",
    "curl -s  --header \"Accept: text/html\"   https://samples.earth/id/documents/c1pnht3h2h44frv6igfg | sed -n '/<script type=\\\"application\\/ld+json\\\">/,/<\\/script>/p' | sed 's/<\\/script>//' | sed 's/<script type=\\\"application\\/ld+json\\\">//' | jsonld format -q\n",
    "```\n",
    "\n",
    "If all goes well we should see the following output.\n",
    "\n",
    "```\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/Dataset> .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/description> \"of data assignments.\" .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/distribution> _:b0 .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/maintainer> <https://samples.earth> .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/name> \"Fake: technical and\" .\n",
    "<https://samples.earth> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/Organization> .\n",
    "<https://samples.earth> <https://schema.org/description> \"DEMO SITE:  fake data for testing\" .\n",
    "_:b0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/DataDownload> .\n",
    "_:b0 <https://schema.org/contentUrl> \"https://samples.earth/id/documents/c1pnht3h2h44frv6igfg.tsv\" .\n",
    "_:b0 <https://schema.org/encodingFormat> \"text/tab-separated-values\" .\n",
    "```\n",
    "\n",
    "So, this was just a simple set of command line calls to give us a feel for the process.  We have seen a sitemap, the URLs \n",
    "that make it up and pulled back those URLs and parsed the JSON-LD.  We then wen ahead and converted the JSON-LD into abother\n",
    "RDF represetnatikon (triples) that make loading into a graph database easier.\n",
    "\n",
    "We could easily take these commands and roll them in a simple bash script.  This might not be a production level approach, \n",
    "but it's a good exercise and a good way to get started.  We will explore more advanced ways of doing this later.\n",
    "\n",
    "## Python\n",
    "\n",
    "Let's look at how this might be done in Python.  Python is a very popular language and has many solid libraries for working with JSON-LD. Again, we will use this more to get a feel for the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_ld_json(url: str) -> dict:\n",
    "    parser = \"html.parser\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, parser)\n",
    "    return json.loads(\"\".join(soup.find(\"script\", {\"type\":\"application/ld+json\"}).contents))\n",
    "\n",
    "def get_ld_jsonurl(url: str) -> dict:\n",
    "    req = requests.get(url)\n",
    "    return req.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "# from conceptnet5.uri import join_uri, split_uri\n",
    "API_ROOT = 'http://api.conceptnet.io'\n",
    "\n",
    "def short_name(value, max_length=40):\n",
    "    \"\"\"\n",
    "    Convert an RDF value (given as a dictionary) to a reasonable label.\n",
    "    \"\"\"\n",
    "    if value['type'] == 'blank node':\n",
    "        return '_'\n",
    "    elif value['type'] == 'IRI':    \n",
    "        url = value['value']\n",
    "        if '#' in url:\n",
    "            # Show just the fragment of URLs with a fragment\n",
    "            # (it's probably a property name)\n",
    "            return url.split('#')[-1]\n",
    "\n",
    "        # Give URLs relative to the root of our API\n",
    "        if url.startswith(API_ROOT):\n",
    "            short_url = url[len(API_ROOT):]\n",
    "            # If the URL is too long, hide it\n",
    "            if len(short_url) > max_length:\n",
    "                pieces = split_uri(short_url)\n",
    "                return join_uri(pieces[0], '...')\n",
    "            else:\n",
    "                return short_url\n",
    "        else:\n",
    "            return url.split('://')[-1]\n",
    "    else:\n",
    "        # Put literal values in quotes\n",
    "        text = value['value'].replace(':', '')\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + '...'\n",
    "        return '\"{}\"'.format(text)\n",
    "\n",
    "\n",
    "def show_graph(url, size=10):\n",
    "    \"\"\"\n",
    "    Show the graph structure of a ConceptNet API response.\n",
    "    \"\"\"\n",
    "    rdf = jsonld.normalize(url)['@default']\n",
    "    graph = graphviz.Digraph(\n",
    "        strict=False, graph_attr={'size': str(size), 'rankdir': 'LR'}\n",
    "    )\n",
    "    for edge in rdf:\n",
    "        subj = short_name(edge['subject'])\n",
    "        obj = short_name(edge['object'])\n",
    "        pred = short_name(edge['predicate'])\n",
    "        if subj and obj and pred:\n",
    "            # Apply different styles to the nodes based on whether they're\n",
    "            # literals, ConceptNet URLs, or other URLs\n",
    "            if obj.startswith('\"'):\n",
    "                # Literal values\n",
    "                graph.node(obj, penwidth='0')\n",
    "            elif obj.startswith('/'):\n",
    "                # ConceptNet nodes\n",
    "                graph.node(obj, style='filled', fillcolor=\"#ddeeff\")\n",
    "            else:\n",
    "                # Other URLs\n",
    "                graph.node(obj, color=\"#558855\")\n",
    "            graph.edge(subj, obj, label=pred)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a search result\n",
    "\n",
    "Let's do a search at the OIH test web site and then see if we can work with some of the results there.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = get_ld_jsonurl(\"https://oss.geocodes-dev.earthcube.org/citesting/summoned/geocodes_demo_datasets/ce020471830dc75cb1639eae403a883f9072bb60.jsonld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@context\": {\n",
      "    \"@vocab\": \"https://schema.org/\"\n",
      "  },\n",
      "  \"@type\": \"Dataset\",\n",
      "  \"additionalType\": \"http://linked.earth/ontology/core/1.2.0/index-en.html#Dataset\",\n",
      "  \"@id\": \"http://lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html\",\n",
      "  \"identifier\": {\n",
      "    \"@id\": \"https://lipdverse.org/resource/dataset/00ZGzZEY6nx8ep1lDO3z\",\n",
      "    \"@type\": \"PropertyValue\",\n",
      "    \"propertyID\": \"http://linked.earth/ontology#hasDatasetId\",\n",
      "    \"name\": \"lipdverse dataset ID 00ZGzZEY6nx8ep1lDO3z\",\n",
      "    \"value\": \"00ZGzZEY6nx8ep1lDO3z\",\n",
      "    \"url\": \"https://lipdverse.org/resource/dataset/00ZGzZEY6nx8ep1lDO3z\"\n",
      "  },\n",
      "  \"name\": \"Hypkana.Hajkova.2016\",\n",
      "  \"description\": \"This dataset from Hypkana (Europe>Eastern Europe>Slovakia) is derived from a LakeSediment archive, and includes data on temperature, uncertainty, reliable, and Commentregardingreliability1. The data are relevant to the time interval from 13074 to 5070 (BP).\",\n",
      "  \"distribution\": [\n",
      "    {\n",
      "      \"@type\": \"DataDownload\",\n",
      "      \"contentUrl\": \"http://lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.lpd\",\n",
      "      \"encodingFormat\": [\n",
      "        \"application/zip\",\n",
      "        \"http://linked.earth/ontology/core/1.2.0/index-en.html#Dataset\"\n",
      "      ],\n",
      "      \"datePublished\": \"2021-02-05T15:52:17+00:00\"\n",
      "    }\n",
      "  ],\n",
      "  \"url\": \"http://lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html\",\n",
      "  \"version\": \"1.0.1\",\n",
      "  \"keywords\": [\n",
      "    \"temperature\",\n",
      "    \"uncertainty\",\n",
      "    \"reliable\",\n",
      "    \"Commentregardingreliability1\"\n",
      "  ],\n",
      "  \"spatialCoverage\": {\n",
      "    \"@type\": \"Place\",\n",
      "    \"name\": \"Hypkana\",\n",
      "    \"geo\": {\n",
      "      \"@type\": \"GeoCoordinates\",\n",
      "      \"latitude\": 48.9131,\n",
      "      \"longitude\": 22.1636,\n",
      "      \"elevation\": 820\n",
      "    }\n",
      "  },\n",
      "  \"license\": \"CC-BY-4.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_formatted_str = json.dumps(ld, indent=2)\n",
    "print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's big\n",
    "\n",
    "There is a lot here, especially in the keywords section.  That's great, this will give us a lot to work with the graph.  For this demo though it might be nice to parse it down a bit.  We can take advantage of a feature of JSON-LD called \"framing\".  Here will make a JSON-LD frame that allows us to parse out only those elements of the document we want to work with.  Framing is vey powerful, but we will work with a simple frame for now.  One like this:\n",
    "\n",
    "```json\n",
    " {\n",
    "  \"@context\": {\"@vocab\": \"https://schema.org/\"},\n",
    "  \"@explicit\": \"true\",\n",
    "   \"@type\":     \"Dataset\",\n",
    "   \"name\": \"\",\n",
    "   \"description\": \"\",\n",
    "   \"url\": \"\",\n",
    "   \"sameAs\": \"\",\n",
    "}\n",
    "```\n",
    "\n",
    "We will run this frame then look at the results and do a quick visualization using the defined function we placed at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@context\": {\n",
      "    \"@vocab\": \"https://schema.org/\"\n",
      "  },\n",
      "  \"@id\": \"http://lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html\",\n",
      "  \"@type\": \"Dataset\",\n",
      "  \"description\": \"This dataset from Hypkana (Europe>Eastern Europe>Slovakia) is derived from a LakeSediment archive, and includes data on temperature, uncertainty, reliable, and Commentregardingreliability1. The data are relevant to the time interval from 13074 to 5070 (BP).\",\n",
      "  \"name\": \"Hypkana.Hajkova.2016\",\n",
      "  \"sameAs\": null,\n",
      "  \"url\": \"http://lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<graphviz.dot.Digraph at 0x7f1c7c473610>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"720pt\" height=\"120pt\"\n viewBox=\"0.00 0.00 720.00 119.72\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.58 0.58) rotate(0) translate(4 202)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-202 1234.91,-202 1234.91,4 -4,4\"/>\n<!-- schema.org/Dataset -->\n<g id=\"node1\" class=\"node\">\n<title>schema.org/Dataset</title>\n<ellipse fill=\"none\" stroke=\"#558855\" cx=\"999.53\" cy=\"-180\" rx=\"103.18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"999.53\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">schema.org/Dataset</text>\n</g>\n<!-- lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html -->\n<g id=\"node2\" class=\"node\">\n<title>lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"282.07\" cy=\"-99\" rx=\"282.15\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"282.07\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html</text>\n</g>\n<!-- lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;schema.org/Dataset -->\n<g id=\"edge1\" class=\"edge\">\n<title>lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;schema.org/Dataset</title>\n<path fill=\"none\" stroke=\"black\" d=\"M412.95,-114.98C465.47,-121.35 526.68,-128.66 582.15,-135 690.96,-147.43 815.88,-160.81 900.27,-169.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"900.05,-173.22 910.36,-170.79 900.78,-166.26 900.05,-173.22\"/>\n<text text-anchor=\"middle\" x=\"666.15\" y=\"-155.8\" font-family=\"Times,serif\" font-size=\"14.00\">type</text>\n</g>\n<!-- &quot;This dataset from Hypkana (Europe&gt;Easter...&quot; -->\n<g id=\"node3\" class=\"node\">\n<title>&quot;This dataset from Hypkana (Europe&gt;Easter...&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"999.53\" cy=\"-126\" rx=\"231.26\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"999.53\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;This dataset from Hypkana (Europe&gt;Easter...&quot;</text>\n</g>\n<!-- lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;&quot;This dataset from Hypkana (Europe&gt;Easter...&quot; -->\n<g id=\"edge2\" class=\"edge\">\n<title>lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;&quot;This dataset from Hypkana (Europe&gt;Easter...&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M525.3,-108.14C608,-111.26 699.94,-114.73 780.62,-117.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"780.78,-121.29 790.91,-118.16 781.05,-114.29 780.78,-121.29\"/>\n<text text-anchor=\"middle\" x=\"666.15\" y=\"-119.8\" font-family=\"Times,serif\" font-size=\"14.00\">schema.org/description</text>\n</g>\n<!-- &quot;Hypkana.Hajkova.2016&quot; -->\n<g id=\"node4\" class=\"node\">\n<title>&quot;Hypkana.Hajkova.2016&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"999.53\" cy=\"-72\" rx=\"127.28\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"999.53\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;Hypkana.Hajkova.2016&quot;</text>\n</g>\n<!-- lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;&quot;Hypkana.Hajkova.2016&quot; -->\n<g id=\"edge3\" class=\"edge\">\n<title>lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;&quot;Hypkana.Hajkova.2016&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M525.3,-89.86C639.12,-85.56 770.41,-80.61 865.79,-77.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"866.21,-80.5 876.07,-76.62 865.95,-73.5 866.21,-80.5\"/>\n<text text-anchor=\"middle\" x=\"666.15\" y=\"-90.8\" font-family=\"Times,serif\" font-size=\"14.00\">schema.org/name</text>\n</g>\n<!-- &quot;http//lipdverse.org/Temp12k/1_0_2/Hypkan...&quot; -->\n<g id=\"node5\" class=\"node\">\n<title>&quot;http//lipdverse.org/Temp12k/1_0_2/Hypkan...&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"999.53\" cy=\"-18\" rx=\"228.26\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"999.53\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;http//lipdverse.org/Temp12k/1_0_2/Hypkan...&quot;</text>\n</g>\n<!-- lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;&quot;http//lipdverse.org/Temp12k/1_0_2/Hypkan...&quot; -->\n<g id=\"edge4\" class=\"edge\">\n<title>lipdverse.org/Temp12k/1_0_2/Hypkana.Hajkova.2016.html&#45;&gt;&quot;http//lipdverse.org/Temp12k/1_0_2/Hypkan...&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M409.25,-82.88C462.58,-76.2 525.36,-68.51 582.15,-62 670.28,-51.89 769,-41.42 848.19,-33.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"848.65,-36.71 858.24,-32.2 847.94,-29.74 848.65,-36.71\"/>\n<text text-anchor=\"middle\" x=\"666.15\" y=\"-65.8\" font-family=\"Times,serif\" font-size=\"14.00\">schema.org/url</text>\n</g>\n</g>\n</svg>\n"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyld import jsonld\n",
    "import json\n",
    "\n",
    "frame = {\n",
    "  \"@context\": {\"@vocab\": \"https://schema.org/\"},\n",
    "  \"@explicit\": \"true\",\n",
    "   \"@type\":     \"Dataset\",\n",
    "   \"name\": \"\",\n",
    "   \"description\": \"\",\n",
    "   \"url\": \"\",\n",
    "   \"sameAs\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "framed = jsonld.frame(ld, frame)\n",
    "\n",
    "json_formatted_str = json.dumps(framed, indent=2)\n",
    "print(json_formatted_str)\n",
    "\n",
    "show_graph(framed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using these approaches you could explore data from other sources you know are publishing JSON-LD.  You could improve either of the bash script or Python code to loop on the resources and store the results in files or load them directly into a triples store / graph database.  \n",
    "\n",
    "As we continue, we will move on to the Gleaner package that is used by Ocean InfoHub and see some of the edge cases that can be addressed with it and how it fits into a more automated process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "884008db010a4da72f62f471ce341f5399c2c405d8eebc0270fe261741869d85"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}